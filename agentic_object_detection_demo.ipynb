{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Object Detection Demo\n",
    "\n",
    "This notebook demonstrates a prototype of an agentic object detection workflow. It uses a multimodal Large Language Model (LLM) to intelligently guide the object detection process by analyzing image patches and deciding whether to use a traditional vision model, request more context, or skip the patch.\n",
    "\n",
    "The workflow relies on Python modules located in the `src` directory of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to sys.path to allow module imports\n",
    "if 'src' not in sys.path:\n",
    "    sys.path.append(os.path.abspath('src'))\n",
    "if '.' not in sys.path: # ensure root is also in path for notebooks in subdirs if any\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "# Verify by trying to import one of the modules\n",
    "try:\n",
    "    import config\n",
    "    print(\"Successfully imported config module.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing config module: {e}. Ensure 'src' is in sys.path and __init__.py exists in src.\")\n",
    "\n",
    "# Import project modules\n",
    "import image_utils\n",
    "import vision_tool_interface\n",
    "import openrouter_agent\n",
    "# config already imported\n",
    "\n",
    "# Import other necessary libraries\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Before running the cells below, ensure that your OpenRouter API key is correctly set in the `src/config.py` file. \n",
    "\n",
    "You can adjust the following parameters to change the input image, target object classes for detection, image partitioning strategy, and contextual expansion factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User Configurable Parameters ---\n",
    "INPUT_IMAGE_PATH = os.path.join(\"data\", \"input.jpg\") # Or \"data/shark.png\"\n",
    "TARGET_CLASSES = [\"person\", \"car\", \"dog\", \"cat\", \"bird\", \"bicycle\", \"traffic light\", \"stop sign\"] # User can customize\n",
    "NUM_ROWS = 3  # Number of rows for partitioning\n",
    "NUM_COLS = 3  # Number of columns for partitioning\n",
    "EXPANSION_FACTOR = 1.5 # For contextual analysis\n",
    "# --- End User Configurable Parameters ---\n",
    "\n",
    "print(f\"Input image path: {INPUT_IMAGE_PATH}\")\n",
    "print(f\"Target classes: {TARGET_CLASSES}\")\n",
    "print(f\"Partitioning grid: {NUM_ROWS}x{NUM_COLS}\")\n",
    "print(f\"Expansion factor: {EXPANSION_FACTOR}\")\n",
    "\n",
    "# Check if API key is placeholder\n",
    "if config.OPENROUTER_API_KEY == \"YOUR_OPENROUTER_API_KEY_HERE\" or not config.OPENROUTER_API_KEY:\n",
    "    display(Markdown(\"<font color='red'>**ERROR:** Please set your OpenRouter API key in `src/config.py` before running!</font>\"))\n",
    "    # In a real notebook, you might want to raise an error to stop execution\n",
    "    # raise ValueError(\"OpenRouter API key not set in src/config.py\")\n",
    "else:\n",
    "    print(f\"Using OpenRouter Model: {config.OPENROUTER_MULTIMODAL_MODEL}\")\n",
    "    # Check if the specified image file exists\n",
    "    if not os.path.exists(INPUT_IMAGE_PATH):\n",
    "        display(Markdown(f\"<font color='red'>**ERROR:** Input image not found at {INPUT_IMAGE_PATH}. Please ensure it exists.</font>\"))\n",
    "        if os.path.exists(\"data\"):\n",
    "            print(f\"Files in data/ directory: {os.listdir('data')}\")\n",
    "        else:\n",
    "            print(\"Error: data/ directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image\n",
    "\n",
    "Load the specified image using the `image_utils` module and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = None\n",
    "if os.path.exists(INPUT_IMAGE_PATH) and (config.OPENROUTER_API_KEY != \"YOUR_OPENROUTER_API_KEY_HERE\" and config.OPENROUTER_API_KEY) :\n",
    "    print(f\"Loading image: {INPUT_IMAGE_PATH}\")\n",
    "    original_image = image_utils.load_image(INPUT_IMAGE_PATH)\n",
    "    if original_image:\n",
    "        print(\"Image loaded successfully.\")\n",
    "        display(original_image)\n",
    "    else:\n",
    "        print(f\"Failed to load image: {INPUT_IMAGE_PATH}\")\n",
    "else:\n",
    "    if not (config.OPENROUTER_API_KEY != \"YOUR_OPENROUTER_API_KEY_HERE\" and config.OPENROUTER_API_KEY):\n",
    "         print(\"Skipping image load due to missing API key.\")\n",
    "    else:\n",
    "         print(\"Skipping image load as file does not exist or API key is not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition Image\n",
    "\n",
    "Divide the loaded image into a grid of patches. These patches will be individually analyzed by the LLM agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_info = []\n",
    "if original_image:\n",
    "    print(f\"Partitioning image into {NUM_ROWS}x{NUM_COLS} patches...\")\n",
    "    patches_info = image_utils.partition_image(original_image, NUM_ROWS, NUM_COLS)\n",
    "    if patches_info:\n",
    "        print(f\"Image partitioned into {len(patches_info)} patches.\")\n",
    "        # Optionally display a few patches\n",
    "        if len(patches_info) > 0:\n",
    "            print(\"Displaying the first patch as an example:\")\n",
    "            display(patches_info[0]['patch_image'])\n",
    "            print(f\"Coordinates of the first patch: {patches_info[0]['coords']}\")\n",
    "    else:\n",
    "        print(\"Failed to partition image.\")\n",
    "else:\n",
    "    print(\"Skipping image partitioning because the original image was not loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Analysis Loop\n",
    "\n",
    "This is the core of the workflow. Each patch is sent to the multimodal LLM agent. The agent decides whether to:\n",
    "1.  **ANALYZE**: Run a standard object detection model on the patch.\n",
    "2.  **EXPAND_CONTEXT**: Request a larger, contextual patch around the current one for re-analysis.\n",
    "3.  **SKIP**: Ignore the patch if it seems uninteresting or irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_detections = []\n",
    "\n",
    "if not patches_info:\n",
    "    print(\"No patches to process. Ensure image was loaded and partitioned correctly.\")\n",
    "elif config.OPENROUTER_API_KEY == \"YOUR_OPENROUTER_API_KEY_HERE\" or not config.OPENROUTER_API_KEY:\n",
    "    print(\"OpenRouter API key not set. Skipping agentic analysis loop.\")\n",
    "else:\n",
    "    for i, patch_info in enumerate(patches_info):\n",
    "        patch_image = patch_info['patch_image']\n",
    "        patch_coords = patch_info['coords'] # (left, upper, right, lower) relative to original\n",
    "        \n",
    "        print(f\"\\n--- Processing Patch {i+1}/{len(patches_info)} - Coords: {patch_coords} ---\")\n",
    "\n",
    "        # Construct initial prompt for the LLM agent\n",
    "        prompt = (\n",
    "            f\"You are an object detection assistant. This image patch is from coordinates {patch_coords} of a larger image. \"\n",
    "            f\"Your task is to identify if any of the following target objects might be present in THIS SPECIFIC PATCH: {', '.join(TARGET_CLASSES)}. \"\n",
    "            f\"Based on the visual information in this patch, do you think it's worth running a detailed object detection model on it? \"\n",
    "            f\"Respond with only one of these keywords: 'ANALYZE' if yes, 'SKIP' if no. \"\n",
    "            f\"If the patch is ambiguous (e.g., shows only a small part of a potential object, like a wheel of a car) such that the full object might be outside this patch but nearby, respond with 'EXPAND_CONTEXT'. \"\n",
    "            f\"Your response must be ONLY one of these three keywords.\"\n",
    "        )\n",
    "\n",
    "        # Call the OpenRouter agent\n",
    "        print(f\"  Sending patch to OpenRouter agent (model: {config.OPENROUTER_MULTIMODAL_MODEL})...\")\n",
    "        agent_decision_text = openrouter_agent.get_agent_response(prompt, image=patch_image)\n",
    "        print(f\"  Agent raw decision: '{agent_decision_text}'\")\n",
    "        \n",
    "        # Sanitize and simplify the agent's response\n",
    "        decision = \"SKIP\" # Default\n",
    "        if isinstance(agent_decision_text, str):\n",
    "            cleaned_response = agent_decision_text.strip().upper()\n",
    "            if \"ANALYZE\" in cleaned_response:\n",
    "                decision = \"ANALYZE\"\n",
    "            elif \"EXPAND_CONTEXT\" in cleaned_response:\n",
    "                decision = \"EXPAND_CONTEXT\"\n",
    "            elif \"SKIP\" in cleaned_response: # Ensure SKIP is also checked if it's part of a longer response\n",
    "                decision = \"SKIP\"\n",
    "            else:\n",
    "                print(f\"  Could not parse a clear keyword from agent response, defaulting to SKIP.\")\n",
    "        else:\n",
    "            print(f\"  Agent response was not a string: '{agent_decision_text}'. Defaulting to SKIP.\")\n",
    "\n",
    "        print(f\"  Parsed decision: {decision}\")\n",
    "\n",
    "        if decision == \"ANALYZE\":\n",
    "            print(f\"  Action: ANALYZE - Running object detection on current patch {patch_coords}...\")\n",
    "            # Ensure vision_tool_interface.model and .processor are loaded\n",
    "            if not vision_tool_interface.model or not vision_tool_interface.processor:\n",
    "                print(\"  Error: Vision model or processor not loaded. Skipping detection.\")\n",
    "                # Attempt to reload them (this is a fallback, should be loaded at module import)\n",
    "                try:\n",
    "                    vision_tool_interface.processor = vision_tool_interface.DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "                    vision_tool_interface.model = vision_tool_interface.DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "                    print(\"  Successfully reloaded vision model and processor.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Failed to reload vision model/processor: {e}\")\n",
    "                    continue # Skip to next patch if model can't be loaded\n",
    "\n",
    "            detections = vision_tool_interface.detect_objects(patch_image, TARGET_CLASSES)\n",
    "            if detections:\n",
    "                print(f\"    Found {len(detections)} objects in patch {patch_coords}.\")\n",
    "                for det in detections:\n",
    "                    det['box'][0] += patch_coords[0]  # x_orig = x_patch + patch_left\n",
    "                    det['box'][1] += patch_coords[1]  # y_orig = y_patch + patch_top\n",
    "                    all_detections.append(det)\n",
    "            else:\n",
    "                print(f\"    No objects found in patch {patch_coords} by vision tool.\")\n",
    "\n",
    "        elif decision == \"EXPAND_CONTEXT\":\n",
    "            print(f\"  Action: EXPAND_CONTEXT - Getting contextual patch and running object detection...\")\n",
    "            contextual_patch_img, contextual_coords = image_utils.get_contextual_patch(original_image, patch_coords, EXPANSION_FACTOR)\n",
    "            print(f\"    Contextual patch coords (original image): {contextual_coords}\")\n",
    "            display(Markdown(\"**Contextual Patch for Review:**\"))\n",
    "            display(contextual_patch_img)\n",
    "            \n",
    "            if not vision_tool_interface.model or not vision_tool_interface.processor:\n",
    "                print(\"  Error: Vision model or processor not loaded. Skipping detection.\")\n",
    "                # (Add reload attempt here if necessary, similar to ANALYZE block)\n",
    "                continue\n",
    "\n",
    "            detections = vision_tool_interface.detect_objects(contextual_patch_img, TARGET_CLASSES)\n",
    "            if detections:\n",
    "                print(f\"    Found {len(detections)} objects in contextual patch {contextual_coords}.\")\n",
    "                for det in detections:\n",
    "                    det['box'][0] += contextual_coords[0]  # x_orig = x_context + contextual_patch_left\n",
    "                    det['box'][1] += contextual_coords[1]  # y_orig = y_context + contextual_patch_top\n",
    "                    all_detections.append(det)\n",
    "            else:\n",
    "                print(f\"    No objects found in contextual patch {contextual_coords} by vision tool.\")\n",
    "                \n",
    "        elif decision == \"SKIP\":\n",
    "            print(f\"  Action: SKIP - Skipping detailed analysis for patch {patch_coords}.\")\n",
    "    \n",
    "    print(\"\\nFinished processing all patches.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "If any objects were detected across the analyzed patches, they are drawn onto a copy of the original image. The resulting image is displayed and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not patches_info: # Check if processing was skipped early\n",
    "    print(\"Result display skipped as no patches were processed.\")\n",
    "elif all_detections:\n",
    "    print(f\"\\nTotal objects detected in the image: {len(all_detections)}\")\n",
    "    # Remove duplicate detections (simple version based on box and label)\n",
    "    unique_detections_set = set()\n",
    "    unique_detections = []\n",
    "    for det in all_detections:\n",
    "        # Create a unique key for the detection (box coordinates and label)\n",
    "        # Rounding coordinates to handle minor float differences if any, and converting to int for hashability\n",
    "        # Box is [x,y,w,h]\n",
    "        detection_key = (tuple(int(round(c)) for c in det['box']), det['label'])\n",
    "        if detection_key not in unique_detections_set:\n",
    "            unique_detections_set.add(detection_key)\n",
    "            unique_detections.append(det)\n",
    "    \n",
    "    print(f\"Total unique objects after basic filtering: {len(unique_detections)}\")\n",
    "    all_detections = unique_detections # Use the filtered list\n",
    "\n",
    "    draw_image = original_image.copy()\n",
    "    draw = ImageDraw.Draw(draw_image)\n",
    "    try:\n",
    "        font = ImageFont.load_default(size=15) # Specify a size\n",
    "    except IOError:\n",
    "        print(\"Default font not found or size parameter not supported. Using basic fallback.\")\n",
    "        try:\n",
    "            font = ImageFont.load_default() # Basic fallback\n",
    "        except:\n",
    "            font = None # Absolute fallback\n",
    "\n",
    "    for det in all_detections:\n",
    "        box = det['box']  # Original image coordinates [x, y, w, h]\n",
    "        label = f\"{det['label']}: {det['score']:.2f}\"\n",
    "        \n",
    "        rect = [box[0], box[1], box[0] + box[2], box[1] + box[3]]\n",
    "        draw.rectangle(rect, outline=\"red\", width=3)\n",
    "        \n",
    "        text_x = box[0]\n",
    "        text_y = box[1] - 20 if box[1] - 20 > 0 else box[1] + 5\n",
    "        \n",
    "        if font:\n",
    "            draw.text((text_x, text_y), label, fill=\"red\", font=font)\n",
    "        else:\n",
    "            draw.text((text_x, text_y), label, fill=\"red\") # No font object if it failed to load\n",
    "\n",
    "    display(Markdown(\"**Final Image with Detections:**\"))\n",
    "    display(draw_image)\n",
    "\n",
    "    base_name, ext = os.path.splitext(os.path.basename(INPUT_IMAGE_PATH))\n",
    "    output_image_path = os.path.join(\"data\", f\"notebook_output_{base_name}{ext}\")\n",
    "    \n",
    "    try:\n",
    "        draw_image.save(output_image_path)\n",
    "        print(f\"Processed image with detections saved to: {output_image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo objects were detected in the image after processing all patches.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
